{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43588eb5-1dfc-4e82-8503-940bbbaffbb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RSLC to GCOV\n",
    "- This notebook converts NISAR RSLC to GCOV data by running ISCE3's `gcov.py`.\n",
    "- Uses the `isce3_src` kernel (created using [Create_Environments.ipynb](https://github.com/isce-framework/sds-ondemand/blob/main/environments/Create_Environments.ipynb)).\n",
    "- Can be ran locally (preferably on a GPU instance), or as a PCM job.\n",
    "\n",
    "# Parameters\n",
    "This cell is marked `parameters`, indicating the variables within can substituted when running this notebook via `papermill`.\n",
    "- `data_link`: S3 url to the NISAR RSLC data to be converted into GSLC.\n",
    "- `dem_s3_url`: S3 url to the DEM file to download.\n",
    "- `gpu_enabled`: `1` to run using the GPU, `0` to use CPU instead. **Keep in mind that while disabling the GPU processing allows this notebook to be ran on an instance without a GPU, that does not guarantee the instance this notebook is running on is a non-GPU instance.** To run on a non-GPU instance on PCM, submit the job to a CPU-only queue.\n",
    "- `s3_upload`: `1` to upload the results of this notebook to an S3 bucket, `0` to ignore this.\n",
    "- `focus_config`: The runconfig passed to `focus.py`.\n",
    "\n",
    "### Upload parameters (may be removed later in favor of automatic uploading)\n",
    "- `s3_url`: S3 url to upload to results to.\n",
    "- `key`: Corresponds to `aws_access_key_id` for a short-term access key stored in `~/.aws/credentials`.\n",
    "- `secret`: Corresponds to `aws_secret_access_key` for a short-term access key stored in `~/.aws/credentials`.\n",
    "- `token`: Corresponds to `aws_session_token` for a short-term access key stored in `~/.aws/credentials`.\n",
    "- `region`: Corresponds to `region` for a short-term access key stored in `~/.aws/credentials`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082b6cd-2ee0-4615-b597-9c6d61472b7b",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "data_link = 's3://nisar-st-data-ondemand/ALOS-1-data/RSLC/ALPSRP274410710-L1.0.h5' # string\n",
    "dem_s3_url = 's3://nisar-st-data-ondemand/DEM-static/dem.tiff' # string\n",
    "gpu_enabled = 1 # boolean\n",
    "s3_upload = 1 # boolean\n",
    "s3_url = 's3://nisar-st-data-ondemand/ALOS-1-data/GCOV' # string\n",
    "gcov_config = '' # string\n",
    "key = '' # string\n",
    "secret = '' # string\n",
    "token = '' # string\n",
    "region = '' # string\n",
    "\n",
    "# hysds specifications\n",
    "_time_limit = 86400\n",
    "_soft_time_limit = 86400\n",
    "_disk_usage = '10GB'\n",
    "_submission_type = 'iteration'\n",
    "_label = 'RSLC to GCOV PGE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee849d03-e4d4-40c1-8da0-47740f0ff97a",
   "metadata": {},
   "source": [
    "### Pre-processing of the Parameters to convert numbers or words into `boolean` True and False values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef6453-bb7c-4ab9-8cfb-cd61a46cff92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert boolean parameters because they must be specified as strings\n",
    "try:\n",
    "    if not isinstance(gpu_enabled, bool):\n",
    "        gpu_enabled = int(gpu_enabled) > 0\n",
    "except ValueError:\n",
    "    if instance(gpu_enabled, str):\n",
    "        gpu_enabled = gpu_enabled.lower() == 'true'\n",
    "    else:\n",
    "        gpu_enabled = False\n",
    "print(f'{gpu_enabled=}')\n",
    "\n",
    "try:\n",
    "    if not isinstance(s3_upload, bool):\n",
    "        s3_upload = int(s3_upload) > 0\n",
    "except ValueError:\n",
    "    if isinstance(s3_upload, str):\n",
    "        s3_upload = s3_upload.lower() == 'true'\n",
    "    else:\n",
    "        s3_upload = False\n",
    "print(f'{s3_upload=}')\n",
    "\n",
    "# Extraneous parameters\n",
    "gslc_config = '' # string\n",
    "focus_config = '' # string\n",
    "insar_config = '' # string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f2976-fae9-481d-b8bf-6d35927f685a",
   "metadata": {},
   "source": [
    "# Functions for loading runconfig files and downloading from S3 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea3b83-58cb-4baa-beb1-5ae272466ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import asf_search as asf\n",
    "import boto3\n",
    "import aws_uploader\n",
    "\n",
    "WORKING_DIR = os.getcwd()\n",
    "HOME_DIR = os.environ['HOME']\n",
    "NOTEBOOK_PGE_DIR = os.environ.get('NOTEBOOK_PGE_DIR', WORKING_DIR)\n",
    "ISCE3_BUILD_DIR = os.environ.get('ISCE3_BUILD_DIR', f'{HOME_DIR}/isce3/build')\n",
    "\n",
    "DOWNLOAD_DIR = os.path.join(WORKING_DIR, 'downloads')\n",
    "EXTRACT_DIR = os.path.join(WORKING_DIR, 'alos_data')\n",
    "OUTPUT_DIR = os.path.join(WORKING_DIR, 'output')\n",
    "RSLC_DIR = os.path.join(WORKING_DIR, 'RSLC')\n",
    "GSLC_DIR = os.path.join(WORKING_DIR, 'GSLC')\n",
    "PRODUCT_DIR = os.path.join(WORKING_DIR, 'product_path')\n",
    "\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# RSLC\n",
    "if focus_config == '':\n",
    "    with open(os.path.join(NOTEBOOK_PGE_DIR, '../templates/focus.yaml'), 'r') as f:\n",
    "        FOCUS_YML = yaml.safe_load(f)\n",
    "else:\n",
    "    print('Using custom focus.py run config...')\n",
    "    FOCUS_YML = yaml.safe_load(focus_config)\n",
    "    \n",
    "# GSLC\n",
    "if gslc_config == '':\n",
    "    with open(os.path.join(NOTEBOOK_PGE_DIR, '../templates/gslc.yaml'), 'r') as f:\n",
    "        GSLC_YML = yaml.safe_load(f)\n",
    "else:\n",
    "    print('Using custom gslc.py run config...')\n",
    "    GSLC_YML = yaml.safe_load(gslc_config)\n",
    "    \n",
    "# GCOV\n",
    "if gcov_config == '':\n",
    "    with open(os.path.join(NOTEBOOK_PGE_DIR, '../templates/gcov.yaml'), 'r') as f:\n",
    "        GCOV_YML = yaml.safe_load(f)\n",
    "else:\n",
    "    print('Using custom gcov.py run config...')\n",
    "    GCOV_YML = yaml.safe_load(gcov_config)\n",
    "\n",
    "# INSAR\n",
    "if insar_config == '':\n",
    "    with open(os.path.join(NOTEBOOK_PGE_DIR, '../templates/insar.yaml'), 'r') as f:\n",
    "        INSAR_YML = yaml.safe_load(f)\n",
    "else:\n",
    "    print('Using custom insar.py run config...')\n",
    "    INSAR_YML = yaml.safe_load(insar_config)\n",
    "\n",
    "def download_alos_data(urls: list) -> list:\n",
    "    \"\"\"Downloads ALOS-1 data given by the asf_search URL.\"\"\"\n",
    "    try:\n",
    "        current_dir = os.getcwd()\n",
    "        user_pass_session = asf.ASFSession().auth_with_creds(username, password)\n",
    "        asf.download_urls(urls=urls, path=DOWNLOAD_DIR, session=user_pass_session, processes=2)\n",
    "        files = os.listdir(DOWNLOAD_DIR)\n",
    "        os.chdir(EXTRACT_DIR)\n",
    "        extract_dirs = []\n",
    "        for f in files:\n",
    "            splitext = os.path.splitext(f)\n",
    "            zip_f = os.path.join(DOWNLOAD_DIR, f)\n",
    "            !unzip -o {zip_f}\n",
    "            extract_dirs.append(os.path.abspath(splitext[0]))\n",
    "        os.chdir(current_dir)\n",
    "        print('Extracted:', extract_dirs)\n",
    "        return extract_dirs\n",
    "    except asf.ASFAuthenticationError as e:\n",
    "        print(f'Auth failed: {e}')\n",
    "\n",
    "def download_alos_s3_data(url: str) -> str:\n",
    "    \"\"\"Downloads ALOS-1 data given by an S3 url.\"\"\"\n",
    "    try:\n",
    "        current_dir = os.getcwd()\n",
    "        zip_f = os.path.join(DOWNLOAD_DIR, os.path.basename(urlparse(url).path))\n",
    "        if not os.path.exists(zip_f):\n",
    "            print(f'Downloading {zip_f} from S3 bucket at {url}...')\n",
    "            aws_uploader.AWS.download_s3(url, zip_f)\n",
    "        else:\n",
    "            print(f'{zip_f} already exists, skipping download...')\n",
    "        os.chdir(EXTRACT_DIR)\n",
    "        !unzip -o {zip_f}\n",
    "        extract_dir = os.path.join(EXTRACT_DIR, os.path.basename(os.path.splitext(zip_f)[0]))\n",
    "        os.chdir(current_dir)\n",
    "        if os.path.isdir(extract_dir):\n",
    "            print('Extracted:', extract_dir)\n",
    "        else:\n",
    "            raise ValueError(f'Failed to extract {extract_dir}!')\n",
    "        return extract_dir\n",
    "    except Exception as e:\n",
    "        print(f'Exception caught while downloading ALOS-1 Data from S3: {e}')\n",
    "\n",
    "def download_dem(url: str) -> str:\n",
    "    \"\"\"Downloads a DEM TIFF file given by an S3 url.\"\"\"\n",
    "    try:\n",
    "        tiff_f = os.path.join(DOWNLOAD_DIR, os.path.basename(urlparse(url).path))\n",
    "        if not os.path.exists(tiff_f):\n",
    "            print(f'Downloading {tiff_f} from S3 bucket at {url}...')\n",
    "            aws_uploader.AWS.download_s3(url, tiff_f)\n",
    "        else:\n",
    "            print(f'{tiff_f} already exists, skipping download...')\n",
    "\n",
    "        if os.path.exists(tiff_f):\n",
    "            print('Downloaded DEM:', tiff_f)\n",
    "        else:\n",
    "            raise ValueError(f'Failed to download {tiff_f}!')\n",
    "        return tiff_f\n",
    "    except Exception as e:\n",
    "        print(f'Exception caught while downloading DEM Data from S3: {e}')\n",
    "\n",
    "def write_focus_config(target_path: str, yml_path: str):\n",
    "    \"\"\"Writes a focus.py runconfig with the specified target path.\"\"\"\n",
    "    FOCUS_YML['runconfig']['groups']['worker']['gpu_enabled'] = gpu_enabled\n",
    "    FOCUS_YML['runconfig']['groups']['input_file_group']['input_file_path'] = [target_path]\n",
    "    FOCUS_YML['runconfig']['groups']['product_path_group']['sas_output_file'] = os.path.join(RSLC_DIR, os.path.basename(target_path))\n",
    "    FOCUS_YML['runconfig']['groups']['product_path_group']['sas_config_file'] = yml_path\n",
    "    with open(yml_path, 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(FOCUS_YML, f, default_flow_style=False)\n",
    "        \n",
    "def write_gslc_config(target_path: str, dem: str, yml_path: str) -> str:\n",
    "    \"\"\"Writes a gslc.py runconfig with the specified target path.\"\"\"\n",
    "    output_name = os.path.basename(os.path.splitext(target_path)[0])\n",
    "    output_path = os.path.join(GSLC_DIR, f'{output_name}_GSLC.h5')\n",
    "    GSLC_YML['runconfig']['groups']['worker']['gpu_enabled'] = gpu_enabled\n",
    "    GSLC_YML['runconfig']['groups']['input_file_group']['input_file_path'] = target_path\n",
    "    GSLC_YML['runconfig']['groups']['product_path_group']['sas_output_file'] = output_path\n",
    "    # GSLC_YML['runconfig']['groups']['product_path_group']['sas_config_file'] = yml_path\n",
    "    GSLC_YML['runconfig']['groups']['dynamic_ancillary_file_group']['dem_file'] = dem\n",
    "    with open(yml_path, 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(GSLC_YML, f, default_flow_style=False)\n",
    "    return output_path\n",
    "\n",
    "def write_gcov_config(target_path: str, dem: str, yml_path: str) -> str:\n",
    "    \"\"\"Writes a gcov.py runconfig with the specified target path.\"\"\"\n",
    "    output_name = os.path.basename(os.path.splitext(target_path)[0])\n",
    "    output_path = os.path.join(GSLC_DIR, f'{output_name}_GCOV.h5')\n",
    "    GCOV_YML['runconfig']['groups']['worker']['gpu_enabled'] = gpu_enabled\n",
    "    GCOV_YML['runconfig']['groups']['input_file_group']['input_file_path'] = target_path\n",
    "    GCOV_YML['runconfig']['groups']['product_path_group']['sas_output_file'] = output_path\n",
    "    GCOV_YML['runconfig']['groups']['dynamic_ancillary_file_group']['dem_file'] = dem\n",
    "    with open(yml_path, 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(GCOV_YML, f, default_flow_style=False)\n",
    "    return output_path\n",
    "\n",
    "def write_insar_config(f1: str, f2: str, dem: str, yml_path: str) -> str:\n",
    "    \"\"\"Writes the INSAR runconfig with the two specified RSLCs and a DEM path.\"\"\"\n",
    "    product_name = INSAR_YML['runconfig']['groups']['product_path_group']['sas_output_file']\n",
    "    \n",
    "    INSAR_YML['runconfig']['groups']['worker']['gpu_enabled'] = gpu_enabled\n",
    "    INSAR_YML['runconfig']['groups']['input_file_group']['reference_rslc_file'] = f1\n",
    "    INSAR_YML['runconfig']['groups']['input_file_group']['secondary_rslc_file'] = f2\n",
    "    INSAR_YML['runconfig']['groups']['dynamic_ancillary_file_group']['dem_file'] = dem\n",
    "    \n",
    "    b1 = os.path.basename(os.path.splitext(f1)[0])\n",
    "    b2 = os.path.basename(os.path.splitext(f2)[0])\n",
    "    ret = os.path.join(PRODUCT_DIR, f'{b1}_{b2}_GUNW.h5')\n",
    "    INSAR_YML['runconfig']['groups']['product_path_group']['sas_output_file'] = ret\n",
    "\n",
    "    # product_types = ['rifg', 'runw', 'gunw', 'roff', 'goff']\n",
    "    # for p_type in product_types:\n",
    "    #     p_type_upper = p_type.upper()\n",
    "    #     INSAR_YML['runconfig']['groups']['input_file_group'][f'qa_{p_type}_input_file'] = f'{b1}_{b2}_{p_type_upper}.h5'\n",
    "    \n",
    "    with open(yml_path, 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(INSAR_YML, f, default_flow_style=False)\n",
    "        \n",
    "    return ret\n",
    "\n",
    "def prepend_env_var(env_var: str, val: str) -> str:\n",
    "    \"\"\"Prepends a value to an environment variable without crashing.\"\"\"\n",
    "    if os.environ.get(env_var, '').find(val) == -1:\n",
    "        os.environ[env_var] = val + ':' + os.environ.get(env_var, '')\n",
    "        if os.environ[env_var].endswith(':'):\n",
    "            os.environ[env_var] = os.environ[env_var][:-1]\n",
    "        return os.environ[env_var]\n",
    "\n",
    "print(WORKING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc9737-9416-4223-a9c2-a88a5df61e95",
   "metadata": {},
   "source": [
    "# Run ISCE3 Python Scripts\n",
    "This cell runs the python scripts:\n",
    "- `gcov.py`: Converts NISAR RSLC -> GCOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69116c4-7783-4648-bdbe-ee9a3966b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "# Download the specified L0B data\n",
    "target_path = os.path.join(DOWNLOAD_DIR, os.path.basename(urlparse(data_link).path))\n",
    "if not os.path.exists(target_path):\n",
    "    print(f'Downloading {target_path} from S3 bucket at {data_link}...')\n",
    "    aws_uploader.AWS.download_s3(data_link, target_path)\n",
    "else:\n",
    "    print(f'{target_path} already exists, skipping download...')\n",
    "\n",
    "# Download the DEM locally\n",
    "dem_f = download_dem(dem_s3_url)\n",
    "\n",
    "# Run gslc.py\n",
    "yml_path = os.path.join(OUTPUT_DIR, 'gcov_final.yaml')\n",
    "output_f = write_gcov_config(target_path, dem_f, yml_path)\n",
    "print(f'Executing:\\n    mamba run -n isce3_src python {ISCE3_BUILD_DIR}/packages/nisar/workflows/gcov.py {yml_path} --no-log')\n",
    "!mamba run -n isce3_src python {ISCE3_BUILD_DIR}/packages/nisar/workflows/gcov.py {yml_path} --no-log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b26586-8f27-4f4d-9af9-a23dd29625c0",
   "metadata": {},
   "source": [
    "# Upload Results to S3\n",
    "This cell uploads the files generated from the ISCE3 cell above to the S3 bucket defined in the `parameters` cell. It imports `aws_uploader.py`, which is a local python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ee47c-34a8-4467-940b-4c4a6914532d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if s3_upload:\n",
    "    parsed_url = urlparse(s3_url)\n",
    "    bucket = parsed_url.netloc\n",
    "    path = parsed_url.path\n",
    "    if path.startswith('/'):\n",
    "        path = path[1:]\n",
    "    if not path.endswith('/'):\n",
    "        path += '/'\n",
    "    path = path.replace('//', '/')\n",
    "    s3_object_path = f'{path}{os.path.basename(output_f)}'\n",
    "    s3_object_url = f's3://{bucket}/{s3_object_path}'\n",
    "    print(f'{s3_object_path=}')\n",
    "    print(f'{s3_object_url=}')\n",
    "\n",
    "    # Upload to s3 bucket\n",
    "    if key != '' or secret != '' or token != '' or region != '':\n",
    "        # short term access key\n",
    "        uploader = aws_uploader.AWS(key, secret, token, region)\n",
    "    else:\n",
    "        # uses role based access\n",
    "        uploader = aws_uploader.AWS('', '', '', '', configdir=False)\n",
    "    uploader.upload_file(output_f, bucket, s3_object_path)\n",
    "    print(f'Successfully uploaded {output_f} to {s3_object_url}')\n",
    "else:\n",
    "    print('Flag for manual S3 upload was not set, skipping this step...')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "isce3_src",
   "language": "python",
   "name": "isce3_src"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
